---
title: "Analyzing Relational Contracts with R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev="svg", error=TRUE, warning=FALSE)
library(dplyr)
library(RelationalContracts)
library(dyngame)
library(repgame)

```

```{r include=FALSE, eval=FALSE}

rmarkdown::render("D:/libraries/RelationalHoldup/RelationalContracts/inst/guide/RelationalContracts.Rmd")
```

# Overview

This vignette illustrates how the R package `RelationalContracting` can be used for game theoretic analysis of relationships that are modelled by infinite horizon games. I also want to give a short introduction to the general idea and motivation for relational contracts,  assuming that the reader already had some exposure to game theory.

Furthermore, I want to motivate the idea of repeated negotiation equilibria introduced in my recent paper with Susanne Goldluecke that views relational contracts as informal agreements that are repeatedly newly negotiated during the relationship. This leads to a different equilibrium selection than the traditional selection of Pareto-optimal subgame perfect equilibria. In particular, I want to motivate the concept by the `Vulnerability Paradoxon` and related rather implausible predictions by Pareto-optimal subgame perfect equilibria.

# A Principal-Agent Relationship

One classic application of relational contracting are principal-agent relationships, in which an agent repeatedly provides a service for a principal. Think of the agent being a supplier or employee of the principal.
While the principal would like to ensure approbriate effort and careful service provision by the agent, it is often not possible to verify and enforce the actually chosen effort level by formal contracts and legal means.

The key idea of relational contracting in this context is that repeated interactions allow informal enforcement of appropriate effort levels. For example, parties could threat to terminate the relationship if effort is too low. Also the principal could promise to pay a bonus for good effort. She can be incentivized to indeed carry out this promise by the reward of high effort also in future periods.

Economists use game-theoretic models to better understand the trade-offs arising in relational contracting. While to a large extend research is driven simply by theoretical curiosity, ideally the derived insights can help to improve the design of institutions and agreements to foster cooperative outcomes in the real world.

## A Simple Model

We start with a simple example of an infinitely repeated principal agent game. An agent (player 2) works for a principal (player 1) and can choose every period some work effort $e$ between 0 and 1. The principal's stage game payoff is equal to the agent's effort of $$\pi_1 = e.$$ The agent incurs quadratic effort cost and has payoffs $$\pi_2 = - \frac 1 2 e^2.$$

At the beginning and end of each period, both parties can voluntary transfer money to each other. We follow the common assumption in relational contracting models and assume that both parties are risk-neutral and thus simply add the net transfers to their stage game payoffs.

For example, assume the principal transfers an amount $p_1$ to the agent and the agent transfers nothing. The transfer could be a bonus payment for positive effort by the agent in the previous period. If the agent chooses effort $e$  in the current period, the principals' total payoffs in the current period are then $e - p_1$ and the agent's $- \frac 1 2 e^2 + p_1$

We assume that principal and agent interact for infinitely many periods and discount future payoffs with a discount factor $\delta$ between 0 and 1.

## Solving in R

We can use the R package `RelationalContracts` to find the set of all subgame perfect equilibrium (SPE) payoffs of this repeated principal-agent game. The following code specifies the repeated game
```{r}
library(RelationalContracts)

g = rel_game("Repeated Principal-Agent Game") %>%
  rel_state("x0",
    # Agent's (player 2) action space
    # Discrete effort levels between 0 and 1 
    A2 = list(e=seq(0,1,by=0.01)),
    # Principal (player 1) has no actions 
    # beyond transfers
    A1 = NULL,
    
    # Stage game payoffs
    pi1 = ~ e, # Principal
    pi2 = ~ -(1/2)*e^2 # Agent
  )
```
Note that we always specfiy action sets and payoffs for a particular state. While a repeated game has just a single state, which we have called "x0", our later examples will involve dynamic games with multiple states. We specify the payoffs as a formula starting with `~` that contains the action `e`. 

The following code finds all subgame perfect equilibrium (SPE) payoffs of our repeated game given a discount factor of $\delta=0.2$ and shows this payoff set:
```{r fig.width=5, fig.height=5}
g = rel_spe(g,delta=0.2)
plot_eq_payoff_set(g)
```

Following the usual convention in the repeated games literature, the SPE payoffs are average discounted payoffs. This means one multiplies the sum of discounted payoffs by $1-\delta$. Average discounted payoffs are on the same scale as stage game payoffs. In particular, if a player gets every period the same payoff $\pi_1$, also her average discounted payoff $u_1$ will be equal to $\pi_1$. That is because we then have
$$u_1 = (1-\delta) \sum_{t=0}^\infty \delta^t \pi_{1} =(1-\delta) \frac {\pi_{1}} {1-\delta}  = \pi_{1}$$

The SPE payoff set is computed using the algorithms described in our articles Goldluecke and Kranz (2012, 2018). Under our assumption of risk-neutrality and the possibility of monetary transfers, the SPE payoff set always has this special triangular form (a simplex) in which the Pareto-frontier has a slope of -1.

Not only are there many possible equilibrium payoffs but most payoffs can be implemented with many different equilibria (i.e. different strategy profiles). Yet, as described in our articles, it sufficies to find a particular *optimal simple strategy profile* to find the whole SPE payoff set. The following code gives a summary of this strategy profile:

```{r}
get_spe(g,action.details = TRUE) %>%
  select("ae.e", "a1.e","U","v1","v2")
```
Let us first understand Pareto-optimal subgame perfect equilibria. On the equilibrium path, the agent will always choose the effort level `ae.e=0.39`. This is the highest incentive compatible effort level given the earlier specified discount factor `delta=0.2`. The results don't show the payments that take place on the equilibrium path. But it follows from the theoretical results that in this game it is easiest (i.e. possible for the largest range of discount factors) to implement a desired effort level if the principal reimburses the agent his cost of the equilbrium effort. You can think of this payment as a bonus payment for the effort.

If no other payments were conducted, the corresponding equilibrium payoff would be the lower right point of the equilibrium payoff set. The principal would get the whole surplus `U=ae.e - 0.5*ae.e^2=0.31` and the agent a payoff of 0.

To grant the agent some surplus, one can use incentive compatible upfront payments that the principal transfers to the agent at the beginning of the very first period. One could equivalently think of those upfront payments as a formal contract between principal and agent that grants the agent a fixed wage in addition to the bonus each period. The fixed wage is independent of which effort level is chosen. By modifying this fixed wage, every point on the Pareto-frontier of the Payoff set can be reached. For example, a fixed wage of `0.5*U = 0.157` would split the surplus equally.

If the principal deviates from a required payment, she will be punished by the agent. In our example, one could effectively punish by repeating the stage game Nash equilibrium in which the agent chooses zero effort, `a1.e=0` and no payments are made in all future periods, a so called Nash-reversion grim-trigger punishment. We call `a1.e` the principal's punishment profile. Both principal and agent would then get continuation payoffs of `v1=v2=0`.

Alternatively, the agent could choose zero effort for just one period and one could then give the principal the option to stop the punishment by paying a fine to the agent that must be at least as high as neccessary to make the original deviation not worthwhile. Using such a scheme with fines always allows to implement the harshest possible punishment, which in some games can even be strictly harsher than Nash-reversion grim-trigger. Furthermore such punishments with fines are renegotiation-proof according to Levin's (2003) concept of strong optimality.

The stage game Nash equilibrium that the agent always chooses zero effort and no payments are made is a subgame perfect equilibrium in which both parties get a payoff of 0. This corresponds to the lower-left point on the equilibrium payoff set. One can reach any point below the Pareto-frontier, e.g. by using a public correlation device to mix between a Pareto-efficient equilibrium and this worst equilibrium. Of course, Pareto-dominated equilibrium payoffs can be implemented with many other equilibria, e.g. equilibria that just prescribe lower effort levels on the equilibrium path.

## Studying Different Institutions and Traditional Equilibrium Selection

One can use game theoretic models to study which institutions and formal contracts allow relational contracting to achieve better outcomes. To compare the outcomes under different institutions, it is typically assumed in the relational contracting literature that a relational contract corresponds to a Pareto-optimal subgame perfect equilibrium. The idea is that similar to formal contracts, also informal contracts can be derived by negotiation and coordination between players at the beginning of their relationship. An informal contract just has to obey the incentive compatibilty constraints imposed by a subgame perfect equilibrium. If players can coordinate their behavior, it seems on first sight a quite sensible approximation that players don't leave money on the table and coordinate to pick a Pareto-optimal SPE as relational contract.  

## Effect of Vulnerability 

To perform a study of a different institutional setting, consider the following variant of our original game. The principal has outsourced a crucial service to the agent and at the same time gives the agent the contractual right to boycot the principal by not delivering this crucial service. This means the principal is now very vulnerable to a boycot by the agent.

We model this setting by allowing the agent a negative effort level `boycot` that involves zero cost for the agent but creates a negative payoff for the principal. To easily compare the effect of such a vulnerability, the following code creates a game with two states: in state `x0` we have the original game and in state `xVul` the principal is vulnerable. So far both states are exogenously given and it is not yet possible to move between states.
```{r}
boycot = -1
g = rel_game("Principal-Agent Game with Exogenous Vulnerability") %>%
  rel_state("x0",
    A2 = list(e=seq(0,1,by=0.01)),
    pi1 = ~ e,
    pi2 = ~ -(1/2)*e^2
  ) %>%
  # State in which the principal is vulnerable
  rel_state("xVul",
    A2 = list(e=c(boycot,seq(0,1,by=0.01))),
    pi1 = ~ e,
    pi2 = ~ -(1/2)*pmax(0,e)^2
  ) %>%
  # Solve set of SPE payoffs
  rel_spe(delta=0.2)

get_spe(g,action.details = TRUE) %>%
  select("x", "ae.e", "U","v1","v2")
```

We see that if the principal is vulnerable (xVul) a substantially higher effort level `ae.e=0.86` can be implemented than in the original state x0 with `ae.e=0.39`. Correspondingly, the maximum sum of payoffs  `U` is also larger if the principal is vulnerable. 

The intuition for this result is simple. If the principal is vulnerable, she can be more strongly punished if she deviates from a promised payment to the agent. Thus larger bonus payments by the principal can be incentivized. With larger bonus payments, a larger effort level by the agent can be implemented.

Instead of fixing the discount factor one often compares the minimally required discount factor neccessary to implement the first best outcome. Here the first best outcome is that the agent chooses every period the maximum effort `e=1`. For repeated games, our software can quickly characterize the optimal equilbria for all discount factors.  The following code returns the critical discount factors to implement full effort: 


```{r}
# Solve all repeated games for all discount
# factors assuming the state is fixed
g = rel_solve_all_repgames(g)

# Show results where first best effort
# ae.e==1 is implemented
get_repgames_results(g) %>% 
  filter(ae.e==1) %>%
  select(x, delta_min, ae.e,U,v1=v1_rep, v2=v2_rep)
```

Looking at the column `delta_min`, we see that in the original game (state `x0`), first best effort can be implemented if the discount factor is at least 0.5. Yet, if the principal is vulnerable (state `xVul`)  the first best can already be implemented for all discount factors above 0.25. 

The following code shows the complete sets of SPE payoffs for the two different states for the fixed discount factor $delta=0.2$:
```{r}
plot_eq_payoff_set(g,x=c("xVul","x0"))
```

We see that every SPE payoff in the state `x0` (red area) is Pareto-dominated by some SPE payoff in the state `xVul` (blue area) where the principal is vulnerable. However, only because both players *can* have a strictly higher payoff in state `xVul` than in state `x0`, it might well be that the the principal is worse off in state `xVul`. We see that in state `xVul` there are a lot of SPE payoffs that are much worse for the principal than her lowest payoff in state `x0`.

Imagine that at the beginning of their relationship, players explicitly negotiate their relational contract. More specifically, assume they pick a SPE payoff according to the Nash bargaining solution using the set of SPE payoffs as bargaining set. To specify the outcome of Nash bargaining, we need a disagreement payoff. Assume the disagreement points correspond to the worst SPE equilibrium outcomes in each state, i.e. the lower-right points of the blue and red triangles, respectively. This means in state `xVul` the agent boycots the principal under disagreement and in state `x0` the agent just picks zero effort. Then the Nash bargaining solution is simply the center point on the Pareto-frontier, which is marked with a little dot in the plot. We see that the principal is then worse off if vulnerable. Even though the vulnerability allows more efficient relational contracting, that positive effect is outweighted by the detoriation of a vulnerable principal's bargaining position.

## Endogenous Vulnerability and the Vulnerability Paradox

Consider now a variant of the previous game. The game starts in state `x0`. As long as we stay in `x0` the principal can decide to stay in `x0` make herself permanently and irreversibly vulnerable and move to state `xVul` at zero cost.
```{r}
boycot = -1
g = rel_game("Principal-Agent Game with Endogenous Vulnerability") %>%
  rel_state("x0",
    A1 = list(move=c("stay","vul")),
    A2 = list(e=seq(0,1,by=0.01)),
    pi1 = ~ e,
    pi2 = ~ -(1/2)*e^2
  ) %>%
  rel_transition("x0","xVul",move="vul",prob=1) %>%
  # State in which the principal is vulnerable
  rel_state("xVul",
    A2 = list(e=c(boycot,seq(0,1,by=0.01))),
    pi1 = ~ e,
    pi2 = ~ -(1/2)*pmax(0,e)^2
  ) %>%
  # Solve set of SPE payoffs
  rel_spe(delta=0.2)
```

Formally, in state x0, the principal has now an action `move` and we added the following line
```s
  rel_transition("x0","xVul",move="vul",prob=1) %>%
```
It specifies that if in the state "x0" the principal chooses the `move` `"vul"` then with probability 1 the next state will be "xVul". For all other cases no state transititions are specified, which means by default that one remains in the current state.


Before showing the results for the discount factr $\delta = 0.2$, I would like to add some interactivity with two short Google forms questions:

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSd9eP9YjdWC3k8kFCHmS_dPhPS14_S-Oya1sNXUQbtWkEuuBw/viewform?embedded=true" width="640" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>

<br>
<br>
<br>

Ok, let's take a look at the results:
```{r}
get_spe(g,action.details = TRUE) %>%
  select("x", "ae.move", "ae.e", "U","v1","v2")
```


# Slowly Intensifying Relationships
