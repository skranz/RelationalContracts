---
title: "Fixed Point Failure"
output: html_document
---

```{r setup, include=FALSE}
library(RelationalContracts)
library(RelationalContractsCpp)
library(RTutor)
library(ggplot2)
library(plotly)
RTutor::set.knit.print.opts(html.data.frame=TRUE, table.max.rows=25, table.max.cols=NULL, round.digits=4, signif.digits=4)
knitr::opts_chunk$set(echo = TRUE)

```

## A simple repeated principal agent game

Below we study a simple repeated game in which the agent can choose effort $e\in \{0,1\}$ at effort cost $0.5\cdot e$ and the principal gets payoff $\pi_1=e$. 

```{r}
library(RelationalContracts)
g = rel_game("Principal-Agent") %>%
  rel_state("x0",A2=list(e=c(0,1)),pi1=~e, pi2=~ -0.5*e) %>%
  rel_compile() %>%
  rel_solve_repgames()

g$rep.games.df
```

As you can see, it is only possible to induce positive effort if the (adjusted) discount factor is above 0.5.

Below we solve a capped version of the game. From period T=20 onwards, we assume that the SPE payoff set of the truncated game is a fixed with U=0.5 and v1=v2=0. We set an adjusted discounted slightly below 0.5 and consider a negotiation probability of rho=0.4.
We solve it via backward induction. Each backward induction step is very similar to each fixed point iteration in APS. 

```{r}
g = rel_game("Principal-Agent") %>%
  # Initial State
  rel_state("x0",A2=list(e=c(0,1)),pi1=~e, pi2=~ -0.5*e) %>%
  rel_after_cap_payoffs(U=0.5,v1=0,v2=0) %>%
  rel_capped_rne(T=20,adjusted.delta = 0.48, rho=0.4, save.history = TRUE)

g$rne.history
```

You see how the payoff set shrinks towards 0 with each iteration set. We can illustrate, the dynamics of the payoff set via an animation. Press the play button below the following plot.

```{r}
# Show dynamic of payoff sets of truncated games
# Backward induction starting from t=T
animate_capped_rne_history(g)
```

In many examples, we have nice convergence of that payoff set. Unfortunately, that is not the case in all examples. A counter-example is below.

## A game without convergence

Below is a variant of the principal agent game with a second state "x1". If in state "x0", the agent chooses effort e=1, we move for one period to state "x1", where both players get a payoff of 0.3. Next period, we are back to state "x0". This can provide additional incentives to choose effort.

```{r}
g = rel_game("Principal-Agent Variation") %>%
  rel_state("x0",A2=list(e=c(0,1)),pi1=~ e, pi2=~ -0.5*e) %>%
  rel_state("x1",pi1=0.3, pi2=0.3) %>%
  rel_transition("x0","x1", e=1) %>%
  rel_transition("x1","x0") %>%
  rel_after_cap_payoffs("x0", U=1,v1=0,v2=0) %>%
  rel_after_cap_payoffs("x1", U=1,v1=0,v2=0) %>%
  rel_capped_rne(T=30,adjusted.delta = 0.48, rho=0, save.history = TRUE)

```

Now take a look at the dynamics of the SPE payoff sets...

```{r}
animate_capped_rne_history(g,x=NULL)
```

Unfortunately, we get no convergence but an oscialation where the payoff sets in states "x0", "x1" grow and shrink.

The following code allows you to study in more detail what happens in the first 10 steps. Note that starting from period 25 (5 before cap), the agent osciliates between effort and no effort in state "x0".
```{r}
hist = filter(g$rne.history, t > max(t)-10)
hist
animate_capped_rne_history(g,x=NULL,hist = hist)
```

Thus it seems that with positive negotiation probabilities, we no longer have general convergence. That is a bit a pity...
